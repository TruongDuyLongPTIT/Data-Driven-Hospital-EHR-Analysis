from config import create_spark_session
from time_normalization import normalize_time
from pyspark.sql.functions import trim, col, concat, date_format, year


def transform_chartevents():
    # T·∫°o Spark Session
    spark = create_spark_session()


    print("üöÄ chartevents table processing...")
    # Table schema
    spark.sql("""DESCRIBE TABLE bronze.chartevents""").show()

    # Load table l√™n Spark DataFrame
    df_chartevents = spark.table("bronze.chartevents")

    # Remove duplicate records
    # df_chartevents = df_chartevents.drop_duplicates() # -> B·∫£ng chartevents n√†y qu√° l·ªõn m√† drop_duplicate() y√™u c·∫ßu ph·∫£i load c·∫£ b·∫£ng l√™n, nh∆∞ng m√† c√°c worker kh√¥ng ƒë·ªß ram, n√™n ch∆∞a bi·∫øt x·ª≠ l√Ω nh∆∞ n√†o

    # X·ª≠ l√Ω NULL
    df_chartevents = df_chartevents.dropna(subset=['subject_id', 'stay_id', 'itemid'])
    df_chartevents = df_chartevents.fillna({'warning': 0})

    # Lo·∫°i b·ªè 1 s·ªë case b·ªã th·ª´a kho·∫£ng tr·∫Øng ƒë·∫ßu/cu·ªëi chu·ªói
    string_cols = ["value", "valueuom"]

    for column_name in string_cols:
        df_chartevents = df_chartevents.withColumn(column_name, trim(col(column_name)))

    # Normalization charttime
    # Vi bo du lieu MIMIC IV bi dich thoi gian nen can chuan hoa lai de cho hop ly 
    # Va tranh khoang thoi gian qua dai, dan den viec sinh ra bang DimTime sau nay qua lon. Khoang thoi gian hien tai la hon 100 nam, voi interval 1 phut / 1 ban ghi thi phai tao ra rat nhieu ban ghi
    # => Do do can chuan hoa lai cho hop ly va tranh khoang thoi gian qua dai

    df_chartevents = df_chartevents.withColumn('charttime', normalize_time('charttime'))
    
    df_chartevents = df_chartevents.withColumn('storetime', normalize_time('storetime'))

    df_chartevents.write \
            .format("iceberg") \
            .mode("overwrite") \
            .saveAsTable("silver.chartevents")
    
    spark.table("silver.chartevents").show()
    print("üìäIn bronze")
    spark.sql("SELECT * FROM bronze.chartevents LIMIT 100").show()
    print("üìäIn silver")
    spark.sql("SELECT * FROM silver.chartevents LIMIT 100").show()
    
    # Test 1: 1 file
    # df1 = spark.read.parquet("bronze.chartevents.data.00000-201-cb7a543f-867b-49df-9296-1a885053ab8f-00001.parquet")
    # df1.dropDuplicates().collect()  # Xem Spark UI -> s·∫Ω th·∫•y √≠t task

    # Test 2: 3 files  
    # df3 = spark.read.parquet(
    #     *[
    #         "s3a://mimic-lakehouse/bronze/chartevents/data/00000-201-cb7a543f-867b-49df-9296-1a885053ab8f-00001.parquet",
    #         "s3a://mimic-lakehouse/bronze/chartevents/data/00001-202-cb7a543f-867b-49df-9296-1a885053ab8f-00001.parquet",
    #         "s3a://mimic-lakehouse/bronze/chartevents/data/00002-203-cb7a543f-867b-49df-9296-1a885053ab8f-00001.parquet"
    #     ]
    # )

    # df3.dropDuplicates().collect()  # Xem Spark UI -> s·∫Ω th·∫•y nhi·ªÅu task h∆°n
    # Shutdown Spark Session
    spark.stop()


def main():
    transform_chartevents()

if __name__ == "__main__":
    main()