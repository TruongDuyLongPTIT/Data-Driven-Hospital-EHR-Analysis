# ğŸ©» [Healthcare] Hospital EHR Data PipelineğŸ©» 
*(The project is being upgraded and refined...)*
- I built a data pipeline project using the **MIMIC dataset** with a **lakehouse architecture**.
- The goal of this project is to **practice and strengthen my data engineering skill**s.
- It demonstrates my ability to design and implement modern data workflows for **real-world healthcare data**.
  
## ğŸ§¬ Bá»™ dá»¯ liá»‡u: MIMIC-IV Dataset

**MIMIC-IV (Medical Information Mart for Intensive Care IV)** lÃ  bá»™ dá»¯ liá»‡u y táº¿ thá»±c táº¿ quy mÃ´ lá»›n, Ä‘Æ°á»£c cÃ´ng bá»‘ bá»Ÿi MIT Lab for Computational Physiology, chá»©a dá»¯ liá»‡u áº©n danh cá»§a hÃ ng chá»¥c nghÃ¬n bá»‡nh nhÃ¢n táº¡i Beth Israel Deaconess Medical Center (Boston, MA).

#### Äáº·c Äiá»ƒm Ná»•i Báº­t

| Äáº·c Äiá»ƒm | MÃ´ Táº£ |
|----------|-------|
| **Dá»¯ Liá»‡u Thá»±c Táº¿** | Dá»¯ liá»‡u lÃ¢m sÃ ng **thá»±c táº¿** tá»« bá»‡nh viá»‡n, khÃ´ng pháº£i dá»¯ liá»‡u tá»•ng há»£p hay mÃ´ phá»ng |
| **Quy MÃ´** | **~100GB** khi lÆ°u trong RDBMS, hÃ ng trÄƒm triá»‡u báº£n ghi |
| **Sá»‘ Bá»‡nh NhÃ¢n** | HÃ ng chá»¥c nghÃ¬n bá»‡nh nhÃ¢n ICU vÃ  ná»™i trÃº |
| **Báº£o Máº­t** | Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c khá»­ Ä‘á»‹nh danh hoÃ n toÃ n (HIPAA compliant) |
| **PhiÃªn Báº£n** | MIMIC-IV v3.1 (latest) |

#### Ná»™i Dung Dataset

**Bá»™ dá»¯ liá»‡u bao gá»“m:**
- **ğŸ¤’ ThÃ´ng tin nhÃ¢n kháº©u há»c cá»§a bá»‡nh nhÃ¢n**: Tuá»•i, giá»›i tÃ­nh, dÃ¢n tá»™c
- **ğŸ©º Cháº©n Ä‘oÃ¡n lÃ¢m sÃ ng**: ICD-9, ICD-10 codes
- **ğŸ’Š KÃª Ä‘Æ¡n thuá»‘c**: Loáº¡i thuá»‘c, liá»u lÆ°á»£ng, thá»i gian
- **ğŸ”¬ XÃ©t nghiá»‡m**: Káº¿t quáº£ lab tests, Ä‘o lÆ°á»ng sinh hiá»‡u
- **ğŸ§ª Thá»§ thuáº­t y táº¿**: Procedures, surgeries
- **ğŸ§« Ghi chÃº lÃ¢m sÃ ng**: Clinical notes (text data)
- **ğŸ©¸ Dá»¯ liá»‡u ICU**: Vital signs, ventilator settings, Ä‘iá»u trá»‹ há»“i sá»©c
- **âš—ï¸ Billing & Insurance**: ThÃ´ng tin dá»‹ch vá»¥ tÃ­nh phÃ­ Ä‘iá»u trá»‹, mÃ£ báº£o hiá»ƒm
- Xem phÃ¢n tÃ­ch chi tiáº¿t bá»™ dá»¯ liá»‡u táº¡i Ä‘Ã¢y: [Documentation](https://colab.research.google.com/drive/14MG0qrJvCDtgT5EgvIRKHGU17OW_T3l0?usp=sharing)

#### Nguá»“n vÃ  tÃ i liá»‡u liÃªn quan:
- Official site: https://physionet.org/content/mimiciv/
- Academic Journal: [Nature Paper](https://www-nature-com.translate.goog/articles/s41597-022-01899-x?error=cookies_not_supported&code=24abe187-8088-40fc-9ade-eae7426b86a1&_x_tr_sl=en&_x_tr_tl=vi&_x_tr_hl=vi&_x_tr_pto=tc)

## ğŸ©¹ System Architecture
![EHR final drawio](https://github.com/user-attachments/assets/f72fc993-9bac-4335-9c38-6cabdf86b6fa)

## ğŸŒ¡ï¸ Infrastructure

| ThÃ nh Pháº§n | PhiÃªn Báº£n | Container | Ports | Chá»©c NÄƒng ChÃ­nh |
|------------|-----------|-----------|-------|-----------------|
| **Spark Master** | 3.5.0 | `spark-master` | 8080 (UI), 7077 (Master) | Äiá»u phá»‘i cluster, quáº£n lÃ½ workers |
| **Spark Worker 1** | 3.5.0 | `spark-worker-1` | 8081 | Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n tÃ¡n |
| **Spark Worker 2** | 3.5.0 | `spark-worker-2` | 8082 | Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n tÃ¡n |
| **Spark Worker 3** | 3.5.0 | `spark-worker-3` | 8083 | Xá»­ lÃ½ dá»¯ liá»‡u phÃ¢n tÃ¡n |
| **MinIO Node 1** | Latest | `minio1` | 9000 (API), 9001 (Console) | Object storage (S3-compatible) |
| **MinIO Node 2** | Latest | `minio2` | - | Distributed storage node |
| **MinIO Node 3** | Latest | `minio3` | - | Distributed storage node |
| **PostgreSQL** | 15.6 | `postgres_db` | 5432 | Relational database, metadata store |
| **Zookeeper** | 7.5.3 | `zookeeper` | 2181 | Required for Kafka |
| **Kafka** | 7.5.3 | `kafka` | 9092-9093 | Distributed event streaming platform |
| **Debezium** | 2.5 | `debezium` | 8087 | Change Data Capture |
| **Apache Flink** | 15.6 | `flink-jobmanager` | 8086 | Process streaming data |
| **Iceberg REST Catalog** | 0.10.0 | `iceberg-rest` | 8181 | Table format, schema management |
| **Trino** | 435 | `trino` | 8084 | Distributed SQL query engine |
| **Apache Airflow** | 2.8.1 | `airflow_standalone` | 8090 | Workflow orchestration |
| **DBT** | 1.7.8 | `dbt_service` | - | Data transformation framework |

## ğŸ§¬ Tech Stack
| Component | Purpose | Technology |
|-----------|---------|------------|
| **ETL Pipeline** | Data extraction, transformation, loading | Apache Spark, Python |
| **Workflow Orchestration** | Schedule & monitor data pipelines | Apache Airflow, dbt |
| **Query Engine** | High-performance SQL analytics | Trino |
| **Lakehouse** | Scalable object storage | MinIO + Apache Iceberg |
| **Database Source** | Ingest\Extract data source to Lakehouse | PostgreSQL |
| **Visualization** | Business intelligence dashboards | Tableau |

## ğŸ’‰ Repository Structure
<details>
<summary>ğŸ“‹ View Full Directory Tree</summary>
  
```shell
Healthcare-Data-Driven-Hospital-EHR-Analysis/
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                          # ETL Pipeline
â”‚   â”œâ”€â”€ Extract/                         # Data ingestion from MIMIC dataset
â”‚   â”‚   â”œâ”€â”€ ingest_mimic.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ Transform/                       # Data transformation & cleaning
â”‚   â”‚   â”œâ”€â”€ chartevents_transform.py
â”‚   â”‚   â”œâ”€â”€ patients_transform.py
â”‚   â”‚   â”œâ”€â”€ icustays_transform.py
â”‚   â”‚   â”œâ”€â”€ d_items_transform.py
â”‚   â”‚   â””â”€â”€ time_normalization.py
â”‚   â””â”€â”€ Load/                            # Load to Data Warehouse
â”‚       â”œâ”€â”€ dimPatients.py
â”‚       â”œâ”€â”€ dimICUStay.py
â”‚       â”œâ”€â”€ dimTime.py
â”‚       â”œâ”€â”€ dimEventType.py
â”‚       â””â”€â”€ factICUVitalSignEvent.py
â”‚
â”œâ”€â”€ ğŸ“‚ airflow/                          # Workflow orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ ğŸ“‚ data_information/                 # Analysis notebooks
â”‚   â”œâ”€â”€ Data_modeling.ipynb
â”‚   â”œâ”€â”€ PhÃ¢n_tÃ­ch_bá»™_dá»¯_liá»‡u.ipynb
â”‚   â””â”€â”€ Thá»‘ng_kÃª_vÃ _chuáº©n_hÃ³a.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ studyhistory/                     # Research notebooks
â”‚   â”œâ”€â”€ MinIO_Iceberg.ipynb
â”‚   â”œâ”€â”€ csv_gz_to_parquet.ipynb
â”‚   â””â”€â”€ Spark_load_data_parallel.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ spark-jars/                       # Spark dependencies
â”‚   â”œâ”€â”€ aws-java-sdk-bundle-1.12.262.jar
â”‚   â”œâ”€â”€ hadoop-aws-3.3.4.jar
â”‚   â””â”€â”€ iceberg-spark-runtime-3.5_2.12-1.6.0.jar
â”‚
â”œâ”€â”€ ğŸ“‚ trino/                            # Query engine
â”‚   â””â”€â”€ etc/catalog/
â”‚       â”œâ”€â”€ iceberg.properties
â”‚       â””â”€â”€ postgres.properties
â”‚
â”œâ”€â”€ ğŸ“‚ conf/                             # Metastore config
â”‚   â””â”€â”€ metastore-site.xml
â”‚
â”œâ”€â”€ ğŸ“‚ etc/                              # Trino config
â”‚   â”œâ”€â”€ config.properties
â”‚   â”œâ”€â”€ jvm.config
â”‚   â””â”€â”€ catalog/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                             # Raw data & scripts
â”‚   â”œâ”€â”€ Ingest.py
â”‚   â”œâ”€â”€ test_spark.py
â”‚   â””â”€â”€ people.csv
â”‚
â”œâ”€â”€ ğŸ“‚ storage/                          # Data lake storage
â”‚
â”œâ”€â”€ ğŸ“‚ command/                          # Command references
â”‚   â”œâ”€â”€ Lá»‡nh hay dÃ¹ng.txt
â”‚   â””â”€â”€ Trino command.txt
â”‚
â”œâ”€â”€ ğŸ“‚ image/                            # Documentation images
â”‚   â”œâ”€â”€ finaldb.gif
â”‚   â”œâ”€â”€ Spark Master UI.png
â”‚   â”œâ”€â”€ Tableau Dashboard.png
â”‚   â””â”€â”€ Spark load data.drawio.svg
â”‚
â”œâ”€â”€ ğŸ“‚ log/                              # Processing logs
â”‚   â”œâ”€â”€ Ingest file 40 by Spark.txt
â”‚   â””â”€â”€ ingest_full_data_successful.txt
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ start.bat
â”œâ”€â”€ stop.bat
â””â”€â”€ README.md
```

</details>

## ğŸš‘ Getting Started
1. **Download Dataset**
  <pre>
    Download dataset on link: https://physionet.org/content/mimiciv/</pre>
2. **Clone my repository**
  <pre>
    git clone https://github.com/TruongDuyLongPTIT/Healthcare-Data-Driven-Hospital-EHR-Analysis.git</pre>
3. **Download JAR files for Spark**
  <pre>
    aws-java-sdk-bundle-1.12.262.jar
    hadoop-aws-3.3.4.jar
    iceberg-spark-runtime-3.5_2.12-1.6.0.jar</pre>
5. **Start and Stop**
  <pre>
    Double-click start.bat file to start all docker container
    Similar, double-click stop.bat to shut-down all docker containers.</pre>
6. **Run pipeline**
  <pre>
    Now, I running scripts manually.
    But, I will use Airflow to schedule running time (Comming soon...)</pre>
    
## ğŸ©º Guide project


<details>
<summary><b>ğŸ¥ Click Ä‘á»ƒ xem video</b></summary>
<br>
  
https://github.com/user-attachments/assets/1fd65fa1-9d60-4751-9cd0-e76834551e57

https://github.com/user-attachments/assets/7a022613-043b-4254-921c-b646e0f88f99

</video>
</details>


**1. Extract data (Database Source -> Bronze Bucket) and Using Apache Iceberg to manage Parquet files as database-like tables.**
<pre>
    scripts/Extract/config.py
    scripts/Extract/ingest_mimic.py</pre>
**! Converting to streaming extract data (use Debezium, Kafka, Apache Flink)...**


https://github.com/user-attachments/assets/478877cc-fe15-4ede-90f2-0342fce44ab3


**2. Tranform data (Bronze Bucket -> Silver Bucket)**
<pre>
    scripts/Transform/chartevents_transform.py
    scripts/Transform/d_items_transform.py
    scripts/Transform/icustays_transform.py
    scripts/Transform/patients_transform.py
    scripts/Transform/time_normalization.py</pre>
    
**3. Load data (Silver Bucket -> Gold Bucket)**
<pre>
    scripts/Load/dimEventType.py
    scripts/Load/dimICUStay.py
    scripts/Load/dimPatients.py
    scripts/Load/dimTime.py
    scripts/Load/factICUVitalSignEvent
</pre>
! BÃ¢y giá», star schema má»›i chá»‰ cÃ³ 1 sá»‘ báº£ng cÆ¡ báº£n. TÃ´i sáº½ má»Ÿ rá»™ng thÃªm ná»¯a Ä‘á»ƒ khai thÃ¡c tá»‘i Ä‘a giÃ¡ trá»‹ cá»§a bá»™ dá»¯ liá»‡u.

**4. Query for Analytics**

~ TÃ´i dÃ¹ng Trino lÃ m trung gian Ä‘á»ƒ query dá»¯ liá»‡u tá»« Data Model rá»“i Ä‘Æ°a vÃ o Tableau Ä‘á»ƒ lÃ m Dashboard.

![Dashboard](https://github.com/user-attachments/assets/674b9079-7766-4736-aa0e-8871db032fd5)

## ğŸ”œ Check list
- [ ] Set up Debezium for CDC and Kafka for streaming processing.
- [ ] I just do full load at first running time, need update logic ETL data from the second time onwards
- [ ] Set up dbt for manage SQL query
- [ ] Update transform logic 
- [ ] Set up Airflow for orchestration pipeline
- [ ] Logging and monitoring
- [x] Build basic data pipeline

