# ğŸ©» [Healthcare] Hospital EHR Data PipelineğŸ©»
- I built a data pipeline project using the **MIMIC dataset** with a **lakehouse architecture**.
- The goal of this project is to **practice and strengthen my data engineering skill**s.
- It demonstrates my ability to design and implement modern data workflows for **real-world healthcare data**.

## ğŸ›°ï¸ System Architecture
![finalllllllll](https://github.com/user-attachments/assets/58fe9ecb-798a-41b5-b84d-a4990f58ce3c)

## ğŸ—ƒï¸ Repository Structure
```shell
Healthcare-Data-Driven-Hospital-EHR-Analysis/
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                          # ETL Pipeline
â”‚   â”œâ”€â”€ Extract/                         # Data ingestion from MIMIC dataset
â”‚   â”‚   â”œâ”€â”€ ingest_mimic.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ Transform/                       # Data transformation & cleaning
â”‚   â”‚   â”œâ”€â”€ chartevents_transform.py
â”‚   â”‚   â”œâ”€â”€ patients_transform.py
â”‚   â”‚   â”œâ”€â”€ icustays_transform.py
â”‚   â”‚   â”œâ”€â”€ d_items_transform.py
â”‚   â”‚   â””â”€â”€ time_normalization.py
â”‚   â””â”€â”€ Load/                            # Load to Data Warehouse
â”‚       â”œâ”€â”€ dimPatients.py
â”‚       â”œâ”€â”€ dimICUStay.py
â”‚       â”œâ”€â”€ dimTime.py
â”‚       â”œâ”€â”€ dimEventType.py
â”‚       â””â”€â”€ factICUVitalSignEvent.py
â”‚
â”œâ”€â”€ ğŸ“‚ airflow/                          # Workflow orchestration
â”‚   â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ logs/
â”‚   â””â”€â”€ plugins/
â”‚
â”œâ”€â”€ ğŸ“‚ data_information/                 # Analysis notebooks
â”‚   â”œâ”€â”€ Data_modeling.ipynb
â”‚   â”œâ”€â”€ PhÃ¢n_tÃ­ch_bá»™_dá»¯_liá»‡u.ipynb
â”‚   â””â”€â”€ Thá»‘ng_kÃª_vÃ _chuáº©n_hÃ³a.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ studyhistory/                     # Research notebooks
â”‚   â”œâ”€â”€ MinIO_Iceberg.ipynb
â”‚   â”œâ”€â”€ csv_gz_to_parquet.ipynb
â”‚   â””â”€â”€ Spark_load_data_parallel.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ spark-jars/                       # Spark dependencies
â”‚   â”œâ”€â”€ aws-java-sdk-bundle-1.12.262.jar
â”‚   â”œâ”€â”€ hadoop-aws-3.3.4.jar
â”‚   â””â”€â”€ iceberg-spark-runtime-3.5_2.12-1.6.0.jar
â”‚
â”œâ”€â”€ ğŸ“‚ trino/                            # Query engine
â”‚   â””â”€â”€ etc/catalog/
â”‚       â”œâ”€â”€ iceberg.properties
â”‚       â””â”€â”€ postgres.properties
â”‚
â”œâ”€â”€ ğŸ“‚ conf/                             # Metastore config
â”‚   â””â”€â”€ metastore-site.xml
â”‚
â”œâ”€â”€ ğŸ“‚ etc/                              # Trino config
â”‚   â”œâ”€â”€ config.properties
â”‚   â”œâ”€â”€ jvm.config
â”‚   â””â”€â”€ catalog/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                             # Raw data & scripts
â”‚   â”œâ”€â”€ Ingest.py
â”‚   â”œâ”€â”€ test_spark.py
â”‚   â””â”€â”€ people.csv
â”‚
â”œâ”€â”€ ğŸ“‚ storage/                          # Data lake storage
â”‚
â”œâ”€â”€ ğŸ“‚ command/                          # Command references
â”‚   â”œâ”€â”€ Lá»‡nh hay dÃ¹ng.txt
â”‚   â””â”€â”€ Trino command.txt
â”‚
â”œâ”€â”€ ğŸ“‚ image/                            # Documentation images
â”‚   â”œâ”€â”€ finaldb.gif
â”‚   â”œâ”€â”€ Spark Master UI.png
â”‚   â”œâ”€â”€ Tableau Dashboard.png
â”‚   â””â”€â”€ Spark load data.drawio.svg
â”‚
â”œâ”€â”€ ğŸ“‚ log/                              # Processing logs
â”‚   â”œâ”€â”€ Ingest file 40 by Spark.txt
â”‚   â””â”€â”€ ingest_full_data_successful.txt
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ start.bat
â”œâ”€â”€ stop.bat
â””â”€â”€ README.md
```
